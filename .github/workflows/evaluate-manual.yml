name: Evaluate Model and Upload Reports

on:
  workflow_dispatch: {}

jobs:
  evaluate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Preview dataset (debug)
        run: |
          echo "Contents of test dataset:"
          head -n 15 data/amazon_balanced_test.csv

      - name: Evaluate model
        env:
          EVAL_DATA_CSV: data/amazon_balanced_test.csv
          EVAL_TEXT_COL: text
          EVAL_LABEL_COL: sentiment
          EVAL_MODEL_PATH: models/logistic_model.pkl
          EVAL_VECTORIZER_PATH: models/tfidf_vectorizer.pkl
        run: |
          mkdir -p reports
          python scripts/evaluate_existing_model.py || echo "Evaluation failed but continuing"

      - name: Show reports
        if: always()
        run: |
          echo "--- Contents of reports folder ---"
          ls -la reports || echo "No reports folder found"
          echo "--- metrics preview ---"
          [ -f reports/metrics_existing.json ] && cat reports/metrics_existing.json || echo "No metrics file"

      - name: Upload reports as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: reports/
          if-no-files-found: warn

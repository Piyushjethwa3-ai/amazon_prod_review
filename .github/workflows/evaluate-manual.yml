name: Eval (debug + artifact + commit)

on:
  workflow_dispatch: {}

# If your project lives inside a subfolder (e.g., amazon_prod_review-main),
# uncomment the defaults below and set that folder so every step runs there.
# defaults:
#   run:
#     working-directory: amazon_prod_review-main

jobs:
  eval:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # allow pushing reports/ to the repo

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Show workspace (top-level)
        run: |
          echo "PWD:"
          pwd
          echo "--- TOP LEVEL LS ---"
          ls -la
          echo "--- SEARCH for key files ---"
          find . -maxdepth 3 -type f \( -name "requirements.txt" -o -name "evaluate_existing_model.py" -o -name "amazon_balanced_test*.csv" \) -print

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Sanity-check script + data presence
        run: |
          test -f scripts/evaluate_existing_model.py || (echo "Missing scripts/evaluate_existing_model.py" && exit 1)
          test -f data/amazon_balanced_test.csv || (echo "Missing data/amazon_balanced_test.csv" && exit 1)
          test -f models/logistic_model.pkl || (echo "Missing models/logistic_model.pkl" && exit 1)
          # vectorizer is needed if your model is NOT a Pipeline
          if [ ! -f models/tfidf_vectorizer.pkl ]; then
            echo "(Note) models/tfidf_vectorizer.pkl not found â€” OK if your model is a Pipeline"
          fi

      - name: Run evaluation (writes to reports/)
        env:
          # Set your real column names here if different:
          EVAL_DATA_CSV: data/amazon_balanced_test.csv
          EVAL_TEXT_COL: text
          EVAL_LABEL_COL: sentiment
          EVAL_MODEL_PATH: models/logistic_model.pkl
          EVAL_VECTORIZER_PATH: models/tfidf_vectorizer.pkl
        run: |
          set -euxo pipefail
          mkdir -p reports
          python scripts/evaluate_existing_model.py
          echo "--- reports/ after evaluation ---"
          ls -la reports || true
          echo "--- show generated files ---"
          [ -f reports/metrics_existing.json ] && tail -n +1 reports/metrics_existing.json | head -n 40 || echo "metrics_existing.json MISSING"
          [ -f reports/predictions.csv ] && head -n 5 reports/predictions.csv || echo "predictions.csv MISSING"
          [ -f reports/confusion_matrix_existing.png ] && ls -lh reports/confusion_matrix_existing.png || echo "confusion_matrix_existing.png MISSING"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            reports/metrics_existing.json
            reports/predictions.csv
            reports/confusion_matrix_existing.png
          if-no-files-found: error

      - name: Commit and push reports
        run: |
          set -euxo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          # Force add in case reports/ is gitignored
          git add -f reports/* || true
          git status
          git commit -m "ci: add/update evaluation reports" || echo "No changes to commit"
          git push || echo "Nothing to push (maybe no changes)"
